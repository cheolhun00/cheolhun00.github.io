---
layout: single
title:  "R을 활용한 동적 및 정적 크롤링"
---

#***데이터 추출***  



### 웹 크롤링과 웹 스크래핑이란?
- **웹 크롤링**: 여러 웹 페이지를 탐색하며 데이터를 수집하는 과정
- **웹 스크래핑**: 특정 페이지에서 원하는 데이터를 추출하는 과정

### 1. 네이버의 크롤링 가능 여부 확인
네이버가 크롤링을 허용하는지 확인하려면 `robots.txt` 파일을 확인하면 된다.

```r
library(rvest)

# 네이버 robots.txt 확인
url <- "https://naver.com/robots.txt"
readLines(url)
```

**`Disallow: /` 설정이 있으면 크롤링이 제한될 가능성이 있음.**
**크롤링이 제한될 경우, 네이버 뉴스 API 사용을 고려해야 함.**

---

## 2. 네이버 뉴스 페이지 가져오기

### 2.1 `read_html()`을 사용하여 페이지 로드
크롤링을 위해 `rvest` 패키지를 활용하여 HTML 문서를 불러온다.

```r
# 네이버 뉴스 페이지 URL
url <- "https://news.naver.com/main/ranking/popularDay.naver"

# 페이지 가져오기
page <- read_html(url)

# HTML 구조 확인
print(page)
```

---

## 3. 뉴스 기사 제목과 링크 크롤링

### 3.1 기사 제목 및 링크 추출
기사 제목과 링크를 가져오려면 `html_nodes()`와 `html_text()`, `html_attr()`를 활용한다.

```r
# 기사 제목 가져오기
titles <- page %>%
  html_nodes(".rankingnews_box a") %>%
  html_text(trim = TRUE)

# 기사 링크 가져오기
links <- page %>%
  html_nodes(".rankingnews_box a") %>%
  html_attr("href")

# 네이버 뉴스는 상대 경로이므로 절대 경로로 변환
links <- ifelse(startsWith(links, "/"), paste0("https://news.naver.com", links), links)
```

---

## 4. 크롤링한 데이터 저장

### 4.1 데이터 정리 및 중복 제거
크롤링한 데이터를 `data.frame` 형태로 정리하고 중복을 제거한다.

```r
# 데이터프레임 생성
news_data <- data.frame(제목 = titles, 링크 = links, stringsAsFactors = FALSE)

# 중복 제거
news_data <- news_data[!duplicated(news_data$링크), ]

# 출력 확인
print(news_data)
```

### 4.2 CSV 파일로 저장
데이터를 CSV 파일로 저장하여 활용할 수 있다.

```r
write.csv(news_data, "naver_news.csv", row.names = FALSE)
print("데이터 저장 완료! 'naver_news.csv' 파일을 확인하세요.")
```

---

## 5. HTML 테이블 크롤링 및 저장

### 5.1 HTML 테이블 데이터 크롤링 개요
- 웹사이트에서 `<table>` 태그로 제공되는 데이터를 가져올 때 유용함.
- `html_table()`을 사용하면 테이블 데이터를 쉽게 변환할 수 있음.

### 5.2 위키백과 인구 변화 데이터 크롤링

```r
# rvest 패키지 로드
library(rvest)

# 웹페이지 가져오기
url <- "https://ko.wikipedia.org/wiki/대한민국의_인구_변화"
page <- read_html(url)

# 테이블 가져오기
tables <- page %>%
  html_nodes("table") %>%
  html_table(fill = TRUE)

# 첫 번째 테이블 출력
head(tables[[1]])
```

### 5.3 데이터 정리

```r
# dplyr 패키지 로드
library(dplyr)

# 두 번째 테이블 선택
population_data <- tables[[2]]

# 컬럼명 변경
colnames(population_data) <- c("연도", "총인구", "남성", "여성", "출생률", "사망률")

# 첫 번째 행 제거 (필요 없는 제목 행)
population_data <- population_data[-1, ]

# NA 값 제거
population_data <- na.omit(population_data)

# 정리된 데이터 출력
head(population_data)
```

### 5.4 CSV 파일 저장

```r
write.csv(population_data, "population_data.csv", row.names = FALSE)
print("데이터 저장 완료! 'population_data.csv' 파일을 확인하세요.")
```

---

## 마무리
이제 `rvest` 패키지를 활용하여 네이버 뉴스 기사와 HTML 테이블 데이터를 크롤링하는 방법을 익혔다. 이를 확장하여 다른 웹사이트에서도 데이터를 수집하고 분석할 수 있다.


# 3. API 활용 및 JSON 데이터 처리

## 목표
- `httr` 패키지를 사용하여 웹 API에서 데이터를 가져오는 방법을 익힌다.
- `jsonlite` 패키지를 활용해 JSON 데이터를 처리할 수 있다.
- API를 활용해 데이터를 요청하고, 데이터프레임으로 변환하여 분석할 수 있다.

## 1. API와 웹 크롤링의 차이

| 구분 | 웹 크롤링 | API 활용 |
|------|----------|-----------|
| 데이터 접근 방식 | HTML에서 직접 데이터 추출 | 서버에서 구조화된 데이터 제공 |
| 주요 도구 | `rvest`, `RSelenium` | `httr`, `jsonlite` |
| 속도 | 상대적으로 느림 | 빠름 |
| 안정성 | HTML 변경 시 코드 수정 필요 | 구조가 유지되므로 안정적 |
| 법적 이슈 | `robots.txt` 확인 필요 | 공식 제공 API는 일반적으로 허용됨 |

## 2. API 데이터 요청하기

API에서 데이터를 가져오기 위해 `httr` 패키지를 사용한다. `GET()` 함수를 이용하여 API 요청을 보낼 수 있다.

```r
library(httr)

# API 요청 예시 (네이버 뉴스 API)
url <- "https://openapi.naver.com/v1/search/news.json?query=데이터+분석&display=5"
res <- GET(url, 
           add_headers("X-Naver-Client-Id" = "YOUR_CLIENT_ID",
                       "X-Naver-Client-Secret" = "YOUR_CLIENT_SECRET"))

# 응답 상태 코드 확인
status_code(res)
```

## 3. JSON 데이터 처리하기

API 응답 데이터는 일반적으로 JSON 형식으로 제공되므로 `jsonlite` 패키지를 사용하여 처리한다.

```r
library(jsonlite)

# API 응답을 JSON 형식으로 변환
json_data <- content(res, as = "text")
parsed_data <- fromJSON(json_data)

# 데이터 구조 확인
str(parsed_data)
```

## 4. 데이터 정리 및 저장

가져온 데이터를 정리하고 CSV 파일로 저장할 수 있다.

```r
# 필요한 컬럼만 선택하여 데이터 정리
news_df <- parsed_data$items %>%
  select(title, link, description, pubDate)

# CSV 파일로 저장
write.csv(news_df, "naver_news_api.csv", row.names = FALSE)
```

---

# 4. 페이지네이션 및 로그인 처리

## 목표
- 페이지네이션을 처리하여 여러 페이지에서 데이터를 가져오는 방법을 익힌다.
- 로그인 후 데이터를 크롤링하는 방법을 이해한다.

## 1. 페이지네이션 개념

페이지네이션(Pagination)은 여러 페이지로 나뉘어진 데이터를 가져오는 과정이다. 일반적으로 다음과 같은 방식으로 구현된다.

| 유형 | URL 구조 예시 | 처리 방법 |
|------|--------------|-----------|
| 쿼리 스트링 | `?page=1` → `?page=2` | URL의 숫자 변경 |
| URL 경로 | `/page/1/` → `/page/2/` | URL 패턴 변경 |
| JavaScript 기반 | 버튼 클릭 후 로딩 | `RSelenium` 필요 |

## 2. 네이버 뉴스 다중 페이지 크롤링

```r
library(rvest)
library(dplyr)

# 페이지 범위 설정
base_url <- "https://news.naver.com/main/ranking/popularDay.naver?page="
page_range <- 1:5

# 데이터 저장을 위한 리스트 생성
news_data <- data.frame(제목 = character(), 링크 = character(), stringsAsFactors = FALSE)

for (page in page_range) {
  url <- paste0(base_url, page)
  page_content <- read_html(url)

  titles <- page_content %>%
    html_nodes(".rankingnews_box a") %>%
    html_text(trim = TRUE)
  
  links <- page_content %>%
    html_nodes(".rankingnews_box a") %>%
    html_attr("href") %>%
    paste0("https://news.naver.com", .)
  
  temp_data <- data.frame(제목 = titles, 링크 = links, stringsAsFactors = FALSE)
  news_data <- bind_rows(news_data, temp_data)
}

# 결과 확인 및 CSV 저장
print(news_data)
write.csv(news_data, "naver_news_pages.csv", row.names = FALSE)
```

## 3. 로그인 후 데이터 크롤링 (httr 활용)

로그인이 필요한 사이트에서 데이터를 가져오기 위해 `httr` 패키지를 활용할 수 있다.

```r
library(httr)

# 네이버 로그인 URL (예제, 실제 로그인 URL 확인 필요)
login_url <- "https://nid.naver.com/nidlogin.login"

# 로그인 정보 설정
login_data <- list(
  id = "your_username",
  pw = "your_password"
)

# POST 요청으로 로그인
res <- POST(login_url, body = login_data, encode = "form")

# 응답 상태 코드 확인
print(status_code(res))
```

---

이 코드들은 API와 페이지네이션을 처리하는 데 유용하며, 필요에 따라 적절히 수정하여 사용할 수 있다.

## 5, 자바스크립트 기반 웹사이트 크롤링 (RSelenium)

### 목표
- 정적 크롤링(rvest)으로 데이터를 가져올 수 없는 경우를 이해할 수 있다.
- RSelenium을 활용하여 자바스크립트로 렌더링되는 웹사이트에서 데이터를 크롤링할 수 있다.
- 동적 크롤링을 위한 기본적인 웹 드라이버 사용법을 익힐 수 있다.

### 1. 정적 크롤링(rvest)과 동적 크롤링(RSelenium)의 차이
- rvest는 HTML 소스코드에서 데이터를 직접 추출함.
- RSelenium은 JavaScript 실행 후 동적으로 생성된 데이터도 가져올 수 있음.

| 크롤링 방식 | 설명 | 사용 예시 |
|------------|-----|----------|
| 정적 크롤링 (rvest) | HTML에서 직접 데이터 추출 | 네이버 뉴스, 위키백과 등 |
| 동적 크롤링 (RSelenium) | JavaScript 실행 후 데이터 추출 | 네이버 실시간 검색, 무한 스크롤 페이지 |

---

### 2. RSelenium 설치 및 설정
- RSelenium은 웹 브라우저를 원격으로 조작하는 라이브러리
- 크롤링하려는 웹사이트의 JavaScript 실행이 필요할 경우 사용

#### 설치 명령어
```r
install.packages("RSelenium")
```
#### 필요한 드라이버(Chrome) 다운로드
최신 버전의 Chrome WebDriver를 다운로드해야 함.
[Chrome WebDriver 다운로드](https://sites.google.com/chromium.org/driver/)

---

### 3. RSelenium을 활용한 기본 크롤링
- 웹 드라이버 실행 후 특정 웹사이트 열기
- JavaScript가 실행된 후 페이지에서 데이터 가져오기

#### 코드 예제: RSelenium을 활용한 웹페이지 열기
```r
library(RSelenium)

# 웹 드라이버 실행
rD <- rsDriver(browser = "chrome", chromever = "latest", verbose = FALSE)
remDr <- rD$client

# 특정 웹사이트 열기
remDr$navigate("https://news.naver.com")

# 페이지 제목 가져오기
page_title <- remDr$getTitle()
print(page_title)

# 웹 드라이버 종료
remDr$close()
rD$server$stop()
```

---

### 4. JavaScript로 동적 요소 가져오기 (예: 클릭, 스크롤)
- JavaScript 실행이 필요한 버튼 클릭
- 무한 스크롤 페이지에서 데이터 로드 후 크롤링

#### 코드 예제: 특정 버튼 클릭 후 데이터 가져오기
```r
# 특정 버튼 클릭 (예: 더보기 버튼)
button <- remDr$findElement(using = "css selector", "button.more")
button$click()

# 페이지 로드 대기
Sys.sleep(3)

# 새로운 데이터 가져오기
updated_page <- remDr$getPageSource()
```

#### 코드 예제: 자동 스크롤 다운
```r
for (i in 1:5) {
  remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
  Sys.sleep(2)  # 페이지가 로드될 시간을 줌
}
```

---

## 6. 실전 프로젝트 및 크롤링 자동화

### 목표
- 웹 크롤링 프로젝트를 기획하고 구현할 수 있다.
- 크롤링한 데이터를 저장하고 자동으로 업데이트할 수 있다.
- 스케줄링을 활용하여 크롤링을 주기적으로 실행할 수 있다.

### 1. 실전 프로젝트 개요
- 웹 크롤링을 실제 데이터 분석과 연결  
- 크롤링한 데이터를 정리하여 활용 가능한 형태로 변환  
- 자동화 및 정기 실행을 통해 데이터를 지속적으로 수집  

#### 프로젝트 주제 예시

| 프로젝트 주제 | 크롤링 대상 | 주요 기술 |
|--------------|-----------|----------|
| 뉴스 트렌드 분석 | 네이버 뉴스 | rvest, httr, jsonlite |
| 주식 시세 수집 | 네이버 금융 | rvest, httr |
| 실시간 검색어 분석 | 네이버 데이터랩 | RSelenium |
| 쇼핑몰 가격 변동 추적 | G마켓, 11번가 | RSelenium, dplyr |

---

### 2. 프로젝트 예제: 네이버 뉴스 트렌드 분석
- 네이버 뉴스에서 인기 기사를 매일 크롤링하여 저장하고 분석
- 기사 제목, 링크, 날짜를 수집하여 CSV 파일로 저장  

#### 코드 예제: 네이버 뉴스 인기기사 크롤링 자동화
```r
library(rvest)
library(dplyr)

# 크롤링할 URL
url <- "https://news.naver.com/main/ranking/popularDay.naver"

# 페이지 가져오기
page <- read_html(url)

# 기사 제목과 링크 가져오기
titles <- page %>%
  html_nodes(".rankingnews_box a") %>%
  html_text(trim = TRUE)

links <- page %>%
  html_nodes(".rankingnews_box a") %>%
  html_attr("href") %>%
  paste0("https://news.naver.com", .)

# 날짜 추가
news_data <- data.frame(날짜 = Sys.Date(), 제목 = titles, 링크 = links, stringsAsFactors = FALSE)

# 기존 데이터 불러오기 (있다면 추가 저장)
if (file.exists("naver_news_trend.csv")) {
  old_data <- read.csv("naver_news_trend.csv", stringsAsFactors = FALSE)
  news_data <- bind_rows(old_data, news_data)
}

# CSV로 저장
write.csv(news_data, "naver_news_trend.csv", row.names = FALSE)

print("네이버 뉴스 크롤링 완료! 데이터가 'naver_news_trend.csv'에 저장되었습니다.")
```

---

### 3. 크롤링 자동화 (cronR, taskscheduleR)
- R 스크립트를 자동 실행하여 주기적으로 데이터를 업데이트  
- Windows: `taskscheduleR` / Linux: `cronR` 활용  

#### 코드 예제: cronR을 사용한 자동 실행 설정 (Linux)
```r
library(cronR)

# 크롤링 실행할 R 스크립트 저장
script_path <- "/home/statcomp/naver_news_trend.R"

# 크론 작업 추가 (매일 오전 9시에 실행)
cmd <- cron_rscript(script_path)
cron_add(cmd, frequency = "daily", at = "09:00", id = "naver_news")
```

#### 코드 예제: taskscheduleR을 사용한 자동 실행 설정 (Windows)
```r
library(taskscheduleR)

# 실행할 R 스크립트 경로
script_path <- "C:/Users/statcomp/Documents/naver_news_trend.R"

# 작업 스케줄러에 추가 (매일 오전 9시 실행)
taskscheduler_create(
  taskname = "naver_news_trend",
  rscript = script_path,
  schedule = "DAILY",
  starttime = "09:00"
)
```



