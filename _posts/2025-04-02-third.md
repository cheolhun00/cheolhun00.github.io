

# 3.1 rvest를 활용한 기본 크롤링

## 🎯 학습 목표
- rvest를 활용하여 네이버 뉴스 페이지에서 데이터를 가져올 수 있다.
- HTML 구조를 분석하고, 기사 제목 및 링크를 크롤링할 수 있다.
- 크롤링한 데이터를 정리하여 CSV로 저장할 수 있다.

## 1. 네이버 뉴스 크롤링 개요
### 1) 웹 크롤링(Web Crawling) vs. 웹 스크래핑(Web Scraping)
- **웹 크롤링**: 여러 페이지를 탐색하며 데이터를 수집
- **웹 스크래핑**: 특정 페이지에서 원하는 데이터를 추출

### 2) 네이버 robots.txt 확인 (크롤링 가능 여부 확인)
- 크롬 개발자 도구(Inspect)를 활용한 HTML 구조 분석

#### 🛠 코드 예제: robots.txt 확인
```r
library(rvest)

# 네이버 robots.txt 확인
url <- "https://naver.com/robots.txt"
readLines(url)
```
🔹 `Disallow: /` 설정이 있으면 크롤링이 제한될 수 있음.
🔹 네이버는 웹 크롤링을 제한할 가능성이 높기 때문에 API 활용도 고려해야 함.

## 2. rvest를 활용한 네이버 뉴스 페이지 가져오기
### 1) read_html()을 사용하여 뉴스 페이지 로드
- HTML 문서 구조 파악 (`html_nodes()`, `html_structure()` 활용)

#### 🛠 코드 예제: 네이버 뉴스 페이지 가져오기
```r
# 네이버 뉴스 페이지 URL
url <- "https://news.naver.com/main/ranking/popularDay.naver"

# 페이지 가져오기
page <- read_html(url)

# HTML 구조 확인
print(page)
```

## 3. 네이버 뉴스 기사 제목과 링크 크롤링
### 1) 기사 제목 및 링크 추출
- **기사 제목 추출**: `html_nodes()`와 `html_text(trim = TRUE)` 활용
- **기사 링크 추출**: `html_nodes()`와 `html_attr("href")` 활용

#### 🛠 코드 예제: 기사 제목과 링크 크롤링
```r
# 기사 제목 가져오기
titles <- page %>%
  html_nodes(".rankingnews_box a") %>%  # 정확한 CSS 선택자 사용
  html_text(trim = TRUE)

# 기사 링크 가져오기
links <- page %>%
  html_nodes(".rankingnews_box a") %>%
  html_attr("href")

# 네이버 뉴스는 상대 경로이므로 절대 경로로 변환
links <- ifelse(startsWith(links, "/"), paste0("https://news.naver.com", links), links)
```

## 4. 크롤링한 데이터 저장
### 1) 데이터프레임으로 정리 및 중복 제거
- 데이터프레임 형태로 변환 후 중복 제거

#### 🛠 코드 예제: 데이터 정리 및 CSV 저장
```r
# 중복 제거 (필요할 경우)
news_data <- data.frame(제목 = titles, 링크 = links, stringsAsFactors = FALSE)
news_data <- news_data[!duplicated(news_data$링크), ]  

# 출력 확인
print(news_data)

# CSV 저장
write.csv(news_data, "naver_news.csv", row.names = FALSE)
```

3.2 HTML 테이블 데이터 크롤링 및 저장

🌟 학습 목표
- rvest를 활용하여 HTML 테이블 데이터를 크롤링할 수 있다.
- html_table()을 사용하여 데이터프레임 형태로 변환할 수 있다.
- 크롤링한 데이터를 CSV로 저장하고, 데이터 정리를 수행할 수 있다.

1. HTML 테이블 데이터 크롤링 개요
✅ 웹사이트에서 데이터가 **표 형태(테이블, <table> 태그)**로 제공되는 경우
✅ html_table()을 사용하여 테이블을 쉽게 크롤링 가능
✅ 위키백과(Wikipedia) 또는 네이버 금융, 스포츠 기록 페이지에서 표 데이터 추출 가능

💡 실제 예제 사이트:
- 위키백과 - 대한민국 인구 변화
- 네이버 금융

2. rvest를 활용한 HTML 테이블 가져오기
🔹 read_html()로 웹페이지 로드
🔹 html_nodes("table")을 활용해 테이블 요소 선택
🔹 html_table()을 활용하여 데이터프레임 변환

🛠 코드 예제: 위키백과에서 인구 변화 데이터 크롤링

```r
library(rvest)

# 웹페이지 가져오기
url <- "https://ko.wikipedia.org/wiki/대한민국의_인구_변화"
page <- read_html(url)

# 테이블 가져오기
tables <- page %>%
  html_nodes("table") %>%
  html_table(fill = TRUE)

# 첫 번째 테이블 출력
head(tables[[1]])
```
✅ fill = TRUE 옵션을 사용하면 비어 있는 셀도 자동으로 채워짐
✅ tables[[1]]을 사용하여 첫 번째 테이블 선택

3. 크롤링한 테이블 데이터 정리
✅ dplyr 패키지를 활용해 데이터 정리
✅ 열 이름 수정, 불필요한 행 제거, NA 값 처리

🛠 코드 예제: 데이터 정리

```r
library(dplyr)

# 데이터 선택 (예: 두 번째 테이블)
population_data <- tables[[2]]

# 컬럼명 변경
colnames(population_data) <- c("연도", "총인구", "남성", "여성", "출생률", "사망률")

# 첫 번째 행 제거 (필요 없는 제목 행)
population_data <- population_data[-1, ]

# NA 값 제거
population_data <- na.omit(population_data)

# 정리된 데이터 출력
head(population_data)
```

4. 크롤링한 데이터 CSV로 저장
✅ 크롤링한 데이터를 CSV 파일로 저장하여 분석 가능
✅ write.csv()를 사용하여 저장

🛠 코드 예제: 데이터 CSV 저장

```r
write.csv(population_data, "population_data.csv", row.names = FALSE)

print("데이터 저장 완료! 'population_data.csv' 파일을 확인하세요.")
```
📌 파일 저장 후 실제 CSV 파일을 열어 데이터가 올바르게 저장되었는지 확인하기

# 3.3 API 활용 및 JSON 데이터 처리

## 🎯 학습 목표
- httr 패키지를 활용하여 웹 API에서 데이터를 가져올 수 있다.
- jsonlite 패키지를 사용하여 JSON 데이터를 R에서 처리할 수 있다.
- API 요청을 통해 데이터를 가져오고, 데이터프레임으로 변환하여 분석할 수 있다.

## 1. API와 웹 크롤링의 차이점

| 구분 | 웹 크롤링 | API 데이터 수집 |
|------|------------|----------------|
| 방식 | HTML에서 데이터 추출 | 서버에서 구조화된 데이터(JSON/XML) 제공 |
| 도구 | rvest, RSelenium | httr, jsonlite |
| 속도 | 느림 (페이지 파싱 필요) | 빠름 (바로 데이터 제공) |
| 안정성 | HTML 변경 시 코드 수정 필요 | 안정적 (API 구조가 유지됨) |
| 법적 이슈 | robots.txt 확인 필요 | 일반적으로 허용됨 (API 키 필요할 수도 있음) |

💡 **네이버 뉴스 웹 크롤링과 네이버 뉴스 API 비교**
- **웹 크롤링**: HTML에서 기사 제목과 링크를 가져옴
- **API**: JSON 형식으로 기사 제목, 링크, 본문, 날짜까지 정리된 데이터를 제공

## 2. API 데이터 요청 (httr 사용)

🔹 `httr::GET()`을 사용하여 API 요청
🔹 `content(as = "text")`로 JSON 데이터 확인

### 🛠 코드 예제: 네이버 뉴스 API 요청 (예시)

```r
library(httr)

# API 요청 (실제 사용하려면 네이버 API 키 필요)
url <- "https://openapi.naver.com/v1/search/news.json?query=데이터+분석&display=5"
res <- GET(url, 
           add_headers("X-Naver-Client-Id" = "YOUR_CLIENT_ID",
                       "X-Naver-Client-Secret" = "YOUR_CLIENT_SECRET"))

# 응답 상태 확인
status_code(res)
```
✅ API 요청 시 보안키(Client ID, Secret)가 필요할 수 있음.
✅ `status_code(res) == 200`이면 정상 응답

## 3. JSON 데이터 처리 (jsonlite 사용)

🔹 `content(res, as = "text")`로 JSON 데이터 확인
🔹 `jsonlite::fromJSON()`을 사용하여 데이터프레임으로 변환

### 🛠 코드 예제: JSON 데이터 변환

```r
library(jsonlite)

# API 응답을 JSON 형식으로 변환
json_data <- content(res, as = "text")
parsed_data <- fromJSON(json_data)

# 데이터 구조 확인
str(parsed_data)
```
✅ JSON 데이터는 리스트 형식이므로 `str(parsed_data)`로 구조를 확인

## 4. API 데이터 정리 및 저장

🔹 기사 제목, 링크, 요약 정보 추출
🔹 데이터프레임으로 변환 후 CSV 저장

### 🛠 코드 예제: 뉴스 데이터 정리 및 저장

```r
# 뉴스 데이터 정리
news_df <- parsed_data$items %>%
  select(title, link, description, pubDate)

# CSV 저장
write.csv(news_df, "naver_news_api.csv", row.names = FALSE)

print("네이버 뉴스 API 데이터를 저장했습니다: naver_news_api.csv")
```
✅ `select(title, link, description, pubDate)`로 필요한 컬럼만 선택
✅ `write.csv()`를 사용해 CSV로 저장

### 3.4 페이지네이션 및 로그인 처리

## 🎯 학습 목표
- 페이지네이션(Pagination)을 처리하여 여러 페이지에서 데이터를 크롤링할 수 있다.
- httr을 사용하여 로그인 후 데이터를 가져올 수 있다.
- 네이버 뉴스 또는 다른 사이트에서 다중 페이지 크롤링을 수행할 수 있다.

## 1. 페이지네이션(Pagination) 개념

✅ **페이지네이션(Pagination)이란?**
- 뉴스, 쇼핑몰, 포럼 등에서는 한 페이지에 모든 데이터를 제공하지 않고, 여러 페이지로 나눠서 표시
- 크롤링 시 반복문(for, map())을 활용하여 여러 페이지에서 데이터를 가져와야 함

✅ **페이지네이션 방식 종류**

| 유형 | 예제 URL 구조 | 처리 방법 |
|------|-------------|----------|
| 쿼리 스트링(Query String) | `?page=1` → `?page=2` | URL에서 숫자 변경 |
| URL 경로(Path) | `/page/1/` → `/page/2/` | URL에서 숫자 변경 |
| JavaScript 기반 | 버튼 클릭으로 로딩 | RSelenium 필요 |

## 2. 네이버 뉴스에서 다중 페이지 크롤링 (rvest)

✅ **네이버 뉴스 랭킹 페이지에서 1~5페이지의 기사 제목과 링크를 가져오기**

```r
library(rvest)
library(dplyr)

# 크롤링할 페이지 범위 설정
base_url <- "https://news.naver.com/main/ranking/popularDay.naver?page="
page_range <- 1:5  # 1~5페이지 크롤링

# 기사 제목과 링크를 저장할 데이터프레임 초기화
news_data <- data.frame(제목 = character(), 링크 = character(), stringsAsFactors = FALSE)

# 반복문을 사용한 크롤링
for (page in page_range) {
  url <- paste0(base_url, page)
  page_content <- read_html(url)

  # 기사 제목 가져오기
  titles <- page_content %>%
    html_nodes(".rankingnews_box a") %>%
    html_text(trim = TRUE)

  # 기사 링크 가져오기
  links <- page_content %>%
    html_nodes(".rankingnews_box a") %>%
    html_attr("href") %>%
    paste0("https://news.naver.com", .)

  # 데이터 저장
  temp_data <- data.frame(제목 = titles, 링크 = links, stringsAsFactors = FALSE)
  news_data <- bind_rows(news_data, temp_data)
}

# 데이터 확인
print(news_data)

# CSV로 저장
write.csv(news_data, "naver_news_pages.csv", row.names = FALSE)
```

✅ `paste0(base_url, page)`를 사용하여 페이지 URL 변경  
✅ `for` 루프를 통해 1~5페이지의 데이터를 반복해서 가져옴  
✅ `bind_rows()`를 사용해 데이터를 누적 저장  

## 3. 로그인 및 쿠키 세션 유지 (httr)

✅ **로그인이 필요한 사이트에서 크롤링하는 경우**
- 로그인 후 데이터를 가져오기 위해 쿠키 및 세션 유지 필요
- `httr::POST()`를 활용하여 로그인 처리 후 데이터 요청

## 4. 네이버 로그인 후 데이터 가져오기 (httr)

✅ 로그인 요청 시, ID/PW 입력이 필요하므로 직접 크롤링은 권장되지 않음.
✅ 하지만, API 또는 `httr`을 활용한 로그인 방식 이해 필요

```r
library(httr)

# 네이버 로그인 URL (예제 URL, 실제 로그인 URL 확인 필요)
login_url <- "https://nid.naver.com/nidlogin.login"

# 로그인 정보 설정
login_data <- list(
  id = "your_username",
  pw = "your_password"
)

# POST 요청으로 로그인
res <- POST(login_url, body = login_data, encode = "form")

# 응답 확인
print(status_code(res))

# 로그인 후 세션 유지하고 다른 페이지 요청 가능
```

✅ `POST()` 요청을 사용해 로그인 처리  
✅ 로그인 성공 시, `status_code(res) == 200` 확인

### 3.5 자바스크립트 기반 웹사이트 크롤링 (RSelenium)

#### 🎯 학습 목표
- 정적 크롤링(rvest)으로 데이터를 가져올 수 없는 경우를 이해할 수 있다.
- RSelenium을 활용하여 자바스크립트로 렌더링되는 웹사이트에서 데이터를 크롤링할 수 있다.
- 동적 크롤링을 위한 기본적인 웹 드라이버 사용법을 익힐 수 있다.

### 1. 정적 크롤링(rvest)과 동적 크롤링(RSelenium)의 차이
✅ rvest는 HTML 소스코드에서 데이터를 직접 추출하지만,
✅ RSelenium은 JavaScript 실행 후 동적으로 생성된 데이터도 가져올 수 있음.

| 크롤링 방식 | 설명 | 사용 예시 |
|------------|-----|----------|
| 정적 크롤링 (rvest) | HTML에서 직접 데이터 추출 | 네이버 뉴스, 위키백과 등 |
| 동적 크롤링 (RSelenium) | JavaScript 실행 후 데이터 추출 | 네이버 실시간 검색, 무한 스크롤 페이지 |

---

### 2. RSelenium 설치 및 설정
✅ RSelenium은 웹 브라우저를 원격으로 조작하는 라이브러리
✅ 크롤링하려는 웹사이트의 JavaScript 실행이 필요할 경우 사용

#### 🔹 설치 명령어
```r
install.packages("RSelenium")
```
#### 🔹 필요한 드라이버(Chrome) 다운로드
최신 버전의 Chrome WebDriver를 다운로드해야 함.
[Chrome WebDriver 다운로드](https://sites.google.com/chromium.org/driver/)

---

### 3. RSelenium을 활용한 기본 크롤링
✅ 웹 드라이버 실행 후 특정 웹사이트 열기
✅ JavaScript가 실행된 후 페이지에서 데이터 가져오기

#### 🛠 코드 예제: RSelenium을 활용한 웹페이지 열기
```r
library(RSelenium)

# 웹 드라이버 실행
rD <- rsDriver(browser = "chrome", chromever = "latest", verbose = FALSE)
remDr <- rD$client

# 특정 웹사이트 열기
remDr$navigate("https://news.naver.com")

# 페이지 제목 가져오기
page_title <- remDr$getTitle()
print(page_title)

# 웹 드라이버 종료
remDr$close()
rD$server$stop()
```
✅ `rsDriver()`를 실행하여 크롬 브라우저를 자동으로 실행
✅ `remDr$navigate("URL")`을 사용하여 웹사이트 이동
✅ `remDr$getTitle()`을 활용하여 페이지 제목 가져오기

---

### 4. JavaScript로 동적 요소 가져오기 (예: 클릭, 스크롤)
✅ JavaScript 실행이 필요한 버튼 클릭
✅ 무한 스크롤 페이지에서 데이터 로드 후 크롤링

#### 🛠 코드 예제: 특정 버튼 클릭 후 데이터 가져오기
```r
# 특정 버튼 클릭 (예: 더보기 버튼)
button <- remDr$findElement(using = "css selector", "button.more")
button$click()

# 페이지 로드 대기
Sys.sleep(3)

# 새로운 데이터 가져오기
updated_page <- remDr$getPageSource()
```
✅ `findElement(using = "css selector", "버튼_클래스")`로 버튼 요소 찾기
✅ `$click()`을 사용하여 버튼 클릭
✅ `Sys.sleep(3)`을 사용해 페이지가 완전히 로드될 때까지 대기

---

### 5. 무한 스크롤 페이지 크롤링
✅ JavaScript가 동작하는 페이지에서 자동으로 스크롤을 내려 새로운 데이터 로드
✅ 예제: 네이버 스포츠 기사 페이지에서 자동 스크롤 실행

#### 🛠 코드 예제: 자동 스크롤 다운
```r
for (i in 1:5) {
  remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
  Sys.sleep(2)  # 페이지가 로드될 시간을 줌
}
```
✅ `executeScript("window.scrollTo(0, document.body.scrollHeight);")`를 실행하여 자동 스크롤
✅ `Sys.sleep(2)`을 사용하여 스크롤 후 페이지 로딩 대기

---

이제 JavaScript 기반 웹페이지에서 데이터를 크롤링하는 실습을 진행하면 돼! 🚀

### 3.6 실전 프로젝트 및 크롤링 자동화

#### 🌟 학습 목표
- 웹 크롤링 프로젝트를 기획하고 구현할 수 있다.
- 크롤링한 데이터를 저장하고 자동으로 업데이트할 수 있다.
- 스케줄링을 활용하여 크롤링을 주기적으로 실행할 수 있다.

---

### 1. 실전 프로젝트 개요

✅ 웹 크롤링을 실제 데이터 분석과 연결  
✅ 크롤링한 데이터를 정리하여 활용 가능한 형태로 변환  
✅ 자동화 및 정기 실행을 통해 데이터를 지속적으로 수집  

#### 🔹 프로젝트 주제 예시

| 프로젝트 주제 | 크롤링 대상 | 주요 기술 |
|--------------|-----------|----------|
| 뉴스 트렌드 분석 | 네이버 뉴스 | rvest, httr, jsonlite |
| 주식 시세 수집 | 네이버 금융 | rvest, httr |
| 실시간 검색어 분석 | 네이버 데이터랩 | RSelenium |
| 쇼핑몰 가격 변동 추적 | G마켓, 11번가 | RSelenium, dplyr |

---

### 2. 프로젝트 예제: 네이버 뉴스 트렌드 분석
✅ 네이버 뉴스에서 인기 기사를 매일 크롤링하여 저장하고 분석  
✅ 기사 제목, 링크, 날짜를 수집하여 CSV 파일로 저장  

#### 🛠 코드 예제: 네이버 뉴스 인기기사 크롤링 자동화

```r
library(rvest)
library(dplyr)

# 크롤링할 URL
url <- "https://news.naver.com/main/ranking/popularDay.naver"

# 페이지 가져오기
page <- read_html(url)

# 기사 제목과 링크 가져오기
titles <- page %>%
  html_nodes(".rankingnews_box a") %>%
  html_text(trim = TRUE)

links <- page %>%
  html_nodes(".rankingnews_box a") %>%
  html_attr("href") %>%
  paste0("https://news.naver.com", .)

# 날짜 추가
news_data <- data.frame(날짜 = Sys.Date(), 제목 = titles, 링크 = links, stringsAsFactors = FALSE)

# 기존 데이터 불러오기 (있다면 추가 저장)
if (file.exists("naver_news_trend.csv")) {
  old_data <- read.csv("naver_news_trend.csv", stringsAsFactors = FALSE)
  news_data <- bind_rows(old_data, news_data)
}

# CSV로 저장
write.csv(news_data, "naver_news_trend.csv", row.names = FALSE)

print("네이버 뉴스 크롤링 완료! 데이터가 'naver_news_trend.csv'에 저장되었습니다.")
```
✅ 기존 데이터(`naver_news_trend.csv`)가 있으면 추가 저장  
✅ `Sys.Date()`를 활용하여 날짜별 데이터 수집  

---

### 3. 크롤링 자동화 (cronR, taskscheduleR)
✅ R 스크립트를 자동 실행하여 주기적으로 데이터를 업데이트  
✅ Windows: `taskscheduleR` / Linux: `cronR` 활용  

#### 4. 크론 작업을 활용한 크롤링 자동 실행 (Linux)

#### 🛠 코드 예제: cronR을 사용한 자동 실행 설정

```r
library(cronR)

# 크롤링 실행할 R 스크립트 저장
script_path <- "/home/statcomp/naver_news_trend.R"

# 크론 작업 추가 (매일 오전 9시에 실행)
cmd <- cron_rscript(script_path)
cron_add(cmd, frequency = "daily", at = "09:00", id = "naver_news")
```
✅ 매일 오전 9시에 자동 실행되는 크론 작업 추가  

---

#### 5. Windows에서 작업 스케줄러를 활용한 자동 실행

#### 🛠 코드 예제: taskscheduleR을 사용한 자동 실행 설정

```r
library(taskscheduleR)

# 실행할 R 스크립트 경로
script_path <- "C:/Users/statcomp/Documents/naver_news_trend.R"

# 작업 스케줄러에 추가 (매일 오전 9시 실행)
taskscheduler_create(
  taskname = "naver_news_trend",
  rscript = script_path,
  schedule = "DAILY",
  starttime = "09:00"
)
```
✅ Windows 작업 스케줄러를 활용하여 매일 오전 9시에 자동 실행  

---

이제 크롤링한 데이터를 자동으로 수집하고 업데이트할 수 있습니다! 🚀


